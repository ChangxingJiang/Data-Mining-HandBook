# 《统计学习方法》啃书辅助：附录A 梯度下降法

> 一点说明：在李航老师《统计学习方法》的附录A中，描述的“梯度下降法”实际上是“最速下降法”。

## 一、基本思路

> 【例 1】想象一个场景，我们站在环形山内侧的山坡上，想要前往环形山内的最低点（图1）。但是暮霭沉沉，只能看清周围很小的一块区域，于是我们不得不通过周围这一小块区域的信息来寻找最低点的方向。那么我们怎么走才可以最快地到达最低点呢？
>
> ![图1](https://hbimg.huabanimg.com/baa8438e4cf0de44f7249c7ab90bc7c63f21709825ccc-f2fSyr_fw658/format/webp)
>
> <center>图1</center>

根据常识，我们会很自然地想到：在当前位置查看四周的坡度，找出下坡坡度最大的方向，向着那个方向走一段；到达新的位置后，再根据新位置四周的地形，找出下坡坡度最大的方向走一段；直至我们发现四周都是上坡，已经无法继续下坡为止，那个位置大概率就是环形山内的最低点。

这就是**梯度下降**的基本思路。梯度下降法是一种迭代算法。先选取适当的自变量**初值**（例1中的初始位置），通过不断迭代更新自变量，实现目标函数（例1中的海拔）的**极小化**，直至**收敛**（例1中四周都是上坡的情况）。在迭代的每一步，均选择使目标函数值下降最快的方向，以这个方向更新自变量，从而达到减小函数值的目的。

但是，在确定了迭代方向之后，要如何确定迭代距离呢？有两种基本的思路。

* 方案一：根据目标方向的陡峭程度（**瞬时变化率**）决定移动的距离。越陡峭，则说明距离极值点还有一定的距离，可以多移动一点；越平缓，则说明可能已经很接近极值点了，于是少移动一点。这种思路构成的方法称为**梯度下降法**。
* 方案二：根据**一维搜索**决定移动的距离，即寻找目标函数在移动方向上的极小值，并直接移动到极小值的位置。这种思路构成的方法称为**最速下降法**，也称**最速梯度下降法**。

将例1推广到一般化的情形，于是有：

> 【例 2】假设$f(x)$是定义在$R^n$上具有一阶连续偏导数的函数。求解下列无约束最优化问题：
> $$
> \min_{x \in R^n} \ f(x) \tag{1}
> $$
>
> 其中的$x$是定义在$R^n$上的n维向量，$f(x)$是定义在n维度空间中的多元函数。

## 二、计算迭代方向：导数→偏导数→方向导数→梯度

这在这一节中，我们讨论迭代方向的计算问题。即求出对于指定自变量值，目标函数值下降最快的方向。当目标函数为**一元函数**时，我们使用**导数**计算瞬时变化率。当目标函数为**多元函数**时，我们使用**偏导数**计算沿各个自变量的坐标轴方向的瞬时变化率，使用**方向导数**计算沿任一指定方向的瞬时变化率，并使用**梯度**描述方向导数取得最大值的方向。在讨论中，我们以二元函数为主，而二元以上的多元函数则可以类推。

#### 1. 导数

首先，我们处理相对简单的特殊情况——目标函数为**一元函数**的情况。

高中数学中的**导数**就是计算一元函数的**瞬时变化率**的工具（人教版高中数学选修1-1第三章）。在同济大学的《高等数学》中，对定义在实数域中的一元函数，给出如下**导数**的定义：

> ##### 【定义 1】导数 （来自同济大学《高等数学》第七版上册 P. 75）
> 
> 设函数$y=f(x)$在点$x_0$的某个邻域内有定义，当自变量x在$x_0$处取得增量$\Delta x$（点$x_0+\Delta x$仍在该邻域内）时，相应地，因变量取得增量$\Delta y=f(x_0+\Delta x)-f(x_0)$；如果$\Delta y$与$\Delta x$之比当$\Delta x \rightarrow 0$时的极限存在，那么称函数$y=f(x)$在点$x_0$处可导，并称这个极限为函数$y=f(x)$在点$x_0$处的导数，即为$f'(x_0)$，即
>
> $$
> f'(x_0) = \lim_{\Delta x \rightarrow 0} \frac{\Delta y}{\Delta x} = \lim_{\Delta x \rightarrow 0} \frac{f(x_0+\Delta x)-f(x_0)}{\Delta x} \tag{2}
> $$

我们可以简单的将导数$f'(x_0)$理解为函数值$y=f(x)$在$x = x_0$处的瞬时变化率。

#### 2. 偏导数

下面，我们处理更一般的情况——目标函数为**多元函数**的情况。

在多元函数情况下，我们先讨论相对简单的特殊情况——只有一个自变量取得增量时，因变量所获得的增量（称为**偏增量**）与自变量增量之间的关系。

以二元函数$z=f(x,y)$为例，如果将自变量$y$固定，视作常量，使二元函数变为$x$的一元函数，这个函数对$x$的导数，就称为二元函数$z=f(x,y)$对于$x$的**偏导数**。

在同济大学的《高等数学》中，对二元函数的**偏导数**，给出如下定义：

> ##### 【定义 2】偏导数 （来自同济大学《高等数学》第七版下册 P. 66）
>
> 设函数$z = f(x,y)$在点$(x_0,y_0)$的某一邻域内有定义，当$y$固定在$y_0$而$x$在$x_0$处有增量$\Delta x$时，相应的函数有增量
> $$
> f(x_0+\Delta x,y_0) - f(x_0,y_0) \tag{3}
> $$
> 如果
> $$
> \lim_{\Delta x \rightarrow 0} \frac{f(x_0+\Delta x,y_0) - f(x_0,y_0)}{\Delta x} \tag{4}
> $$
> 存在，那么称此极限为函数$z=f(x,y)$在点$(x_0,y_0)$处对$x$的偏导数，记作$\frac{\partial z}{\partial x}|_{\begin{align} x = x_0 \\ y=y_0 \end{align}}$，$f_x(x_0,y_0)$，或$f'_x(x_0,y_0)$。

我们也可以简单地将偏导数理解为多元函数沿各个自变量的坐标轴方向的瞬时变化率。

我们将$f(x+\Delta x,y) - f(x,y)$、$f(x,y+\Delta y) - f(x,y)$称为二元函数对$x$和对$y$的**偏增量**，$f_x(x,y) \Delta x$、$f_y(x,y) \Delta y$称为二元函数对$x$和对$y$的**偏微分**。

##### 偏导数的几何解释

下面以二元函数$z=f(x,y)$为例，给出偏导数的几何解释。二元函数对$x$的偏导数，可以理解为二元函数（图2中的渐变色曲面）在与$x$轴和$z$轴同时平行的平面（例如图2中的蓝色平面）上随自变量变化的瞬时变化率；在这个平面上，自变量方向就是$x$轴的方向，因变量方向就是$z$轴的方向。

![图2](image-20210419184923487.png)

<center>图2</center>

##### 偏导数的另一种不严谨理解

因为导数是表示一元函数瞬时变化率的工具。因此，对于多元函数来说，我们需要将多个自变量通过多元一次函数，线性变换为一个自变量，进而计算因变量对这个新自变量的导数。偏导数则是要求在构造自变量的多元一次函数时，只有一个自变量的系数不为0，除此以外其他自变量的系数均为0。

#### 3. 全微分

下面我们讨论多元函数情况下，相较于**偏导数**更一般的情况——各个自变量都取得增量时，因变量所获得的增量（称为**全增量**）与自变量增量之间的关系。

相较于偏导数考虑多元函数沿各个自变量的坐标轴方向的瞬时变化率，我们也可以简单地将这种更一般的情况理解为多元函数沿任一指定方向变化的瞬时变化率。

因为直接计算全增量非常复杂，所以我们希望用各个自变量增量的线性函数来近似地代替函数的全增量，从而引入**全微分**的概念。

在同济大学的《高等数学》中，对二元函数的**全微分**，给出如下定义：

> ##### 【定义 3】全微分（来自同济大学《高等数学》第七版下册 P. 72）
>
> 设函数$z=f(x,y)$在点$(x,y)$的某邻域内有定义，如果函数在点$(x,y)$的全增量
> $$
> \Delta z = f(x+\Delta x,y+\Delta y)-f(x,y) \tag{5}
> $$
> 可表示为
> $$
> \Delta z = A \Delta x + B \Delta y + o(\rho) \tag{6}
> $$
> 其中$A$和$B$不依赖于$\Delta x$和$\Delta y$而仅与$x$和$y$有关，$\rho = \sqrt{(\Delta x)^2 + (\Delta y)^2}$，那么称函数$z=f(x,y)$在点$(x,y)$可微分，而$A \Delta x + B \Delta y$称为函数$z=f(x,y)$在点$(x,y)$的全微分，记作$dz$，即
> $$
> dz = A \Delta x + B \Delta y \tag{7}
> $$

其中$o(\rho)$为比$\rho$更高阶的无穷小。当函数$f(x,y)$可微分时，那么当$\Delta y=0$时式(6)也应成立，并且有$\rho=|\Delta x|$，于是式(6)成为
$$
f(x+\Delta x,y) - f(x,y) = A \Delta x + o(|\Delta x|) \tag{8}
$$
将上式两边各自除以$\Delta x$，再令$\Delta x \rightarrow 0$，则有
$$
\lim_{\Delta x \rightarrow 0} \frac{f(x+\Delta x,y)-f(x,y)}{\Delta x} = A \tag{9}
$$
从而偏导数$\frac{\partial z}{\partial x}$存在，且等于A。同理可证$\frac{\partial z}{\partial y}$。于是得到如下定理：

> ##### 【定理 4】函数$z=f(x,y)$在点$(x,y)$可微分的必要条件（来自同济大学《高等数学》第七版下册 P. 73）
>
> 如果函数$z=f(x,y)$在点$(x,y)$可微分，那么该函数在点$(x,y)$的偏导数$\frac{\partial z}{\partial x}$与$\frac{\partial z}{\partial y}$必定存在，且函数$z=f(x,y)$在点$(x,y)$的全微分为
> 
> $$
> dz = \frac{\partial z}{\partial x} \Delta x + \frac{\partial z}{\partial y} \Delta y \tag{10}
> $$

通过将全增量$\Delta z$拆分为$\Delta z=[f(x+\Delta x,y+\Delta y)-f(x,y+\Delta y)]+[f(x,y+\Delta y)-f(x,y)]$还可以证明如下定理（以下定理的详细证明参见定理5的证明参见同济大学《高等数学》第七版下册 P. 74，此处略）：

> ##### 【定理 5】函数$z=f(x,y)$在点$(x,y)$可微分的充分条件（来自同济大学《高等数学》第七版下册 P. 74）
>
> 如果函数$z=f(x,y)$的偏导数$f_x(x,y)$、$f_y(x,y)$在点$(x,y)$连续，那么函数在该点可微分。

---

简单来说，定义3、定理4和定理5表明：如果二元函数$z=f(x,y)$在某点的偏导数连续，则称函数在该点可微分，全增量可表示为
$$
f(x_0 +\Delta x,y_0+\Delta y) -f(x_0,y_0) = f_x(x_0,y_0) \Delta x + f_y(x_0,y_0) \Delta y + o(\sqrt{(\Delta x)^2 + (\Delta y)^2}) \tag{11}
$$
同时，函数在该点的**全微分**等于它的两个**偏微分**之和，即
$$
dz = \frac{\partial z}{\partial x} \Delta x + \frac{\partial z}{\partial y} \Delta y \tag{12}
$$
于是，我们解决了计算当各个自变量都取得增量时，因变量所获得的增量的计算方法。

#### 4. 方向导数

首先，我们引入**方向余弦**的概念，来描述当前指定的自变量变化方向。

以三元函数为例，我们不妨将三个自变量视作三条坐标轴。非零向量$\bold{r}$与三条坐标轴的夹角$\alpha$、$\beta$、$\gamma$称为向量$\bold{r}$的**方向角**。设$\vec{OM} = \bold{r} = (x,y,z)$，做$M$到第一条坐标轴的垂线，垂线与坐标轴交于点$P$，于是有$MP ⊥ OP$，故$x$为有向线段$\vec{OP}$的长度，于是有

$$
cos \ \alpha = \frac{x}{|OM|} = \frac{x}{|\bold{r}|} \tag{13}
$$

类似可知

$$
cos \ \beta = \frac{y}{|\bold{r}|}, \hspace{1em} cos \ \gamma = \frac{z}{|\bold{r}|} \tag{14}
$$

从而

$$
(cos \ \alpha,cos \ \beta,cos \ \gamma) = (\frac{x}{|\bold{r}|},\frac{y}{|\bold{r}|},\frac{z}{|\bold{r}|}) = \frac{1}{|\bold{r}|} (x,y,z) = \frac{\bold{r}}{|\bold{r}|} = \bold{e}_r \tag{15}
$$

$cos \ \alpha$，$cos \ \beta$，$cos \ \gamma$称为向量$\bold{r}$的**方向余弦**，以向量$\bold{r}$的方向余弦为坐标的向量就是与$\bold{r}$同方向的单位向量$\bold{e}_r$。于是有
$$
cos^2 \alpha + cos^2 \beta + cos^2 \gamma = 1 \tag{16}
$$

---

下面，我们开始处理多元函数沿任一指定方向变化时的瞬时变化率。

以二元函数$z = f(x,y)$为例，假设自变量的变化方向为$l$，$l$是以点$P(x_0,y_0)$为始点的一条射线，我们用以$l$的**方向余弦**为坐标的单位向量$\bold{e}_l = (cos \alpha,cos \beta)$表示这个方向。于是，射线$l$的参数方程可以写作
$$
\begin{cases}
x = x_0 + t \ cos \alpha, \\
y = y_0 + t \ cos \beta
\end{cases}
\hspace{1em} (t \ge 0)
\tag{17}
$$
设$P(x_0 + t \ cos \alpha,y_0 + t \ cos \beta)$为射线$l$上一点。代入式(16)，则自变量增量（$P$到$P_0$的距离）为
$$
\sqrt{(t \ cos \ \alpha)^2+(t \ cos \ \beta)^2} = \sqrt{t^2 (cos^2 \alpha + cos^2 \beta)} = t \tag{18}
$$
于是当$P$沿着$l$趋于$P_0$（即$t \rightarrow 0^+$）时，二元函数的全增量与自变量增量的比值的极限为
$$
\lim_{t \rightarrow 0^+} \frac{f(x_0 + t \ cos \alpha,y_0 + t \ cos \beta)-f(x_0,y_0)}{t} \tag{19}
$$
这个极限称为函数$z=f(x,y)$在点$(x_0,y_0)$处沿方向$l$的**方向导数**，记作$\frac{\partial f}{\partial l}|_{(x_0,y_0)}$。根据方向导数的定义可知，方向导数$\frac{\partial f}{\partial l}|_{(x_0,y_0)}$就是函数$f(x,y)$在点$P_0(x_0,y_0)$处沿方向$l$的变化率。

根据式(11)，当$f(x,y)$在点$(x_0,y_0)$可微分，因为$\Delta x = t \cos \alpha$，$\Delta y = t \cos \beta$，所以方向导数可以写成
$$
\begin{align}
\frac{\partial f}{\partial l}|_{(x_0,y_0)} 
& = \lim_{t \rightarrow 0^+} \frac{f(x_0 + t \ cos \alpha,y_0 + t \ cos \beta)-f(x_0,y_0)}{t} \\
& = \lim_{t \rightarrow 0^+} \frac{f_x(x_0,y_0) \ t \cos \alpha + f_y(x_0,y_0) \ t \cos \beta + o(\sqrt{(t \ cos \ \alpha)^2+(t \ cos \ \beta)^2})}{t} \\
& = f_x(x_0,y_0) \cos \alpha + f_y(x_0,y_0) \cos \beta
\end{align}
\tag{20}
$$
其中$o(\sqrt{(t \ cos \ \alpha)^2+(t \ cos \ \beta)^2})=o(t)$为比$t$更高阶的无穷小。于是得到如下定理：

> ##### 【定理 6】方向导数 （来自同济大学《高等数学》第七版下册 P. 104）
>
> 如果函数$f(x,y)$在点$P_0(x_0,y_0)$可微分，那么函数在该点沿任意方向$l$额方向导数存在，且有
> $$
> \frac{\partial f}{\partial l}|_{(x_0,y_0)} = f_x(x_0,y_0) cos \ \alpha + f_y(x_0,y_0) cos \ \beta \tag{21}
> $$
>
> 其中$cos \ \alpha$和$cos \ \beta$是方向$l$的方向余弦。

于是，我们解决了多元函数沿任一指定方向变化时的瞬时变化率的计算。下面寻找瞬时变化率最大的方向。

#### 5. 梯度

二元函数$z = f(x,y)$为例。如果函数在点$P_0(x_0,y_0)$可微分，$\bold{e}_l = (\cos \alpha,\cos \beta)$是与方向$l$同方向的单位向量，那么函数在点$P_0$沿方向$l$的方向导数
$$
\begin{align}
\frac{\partial f}{\partial l}|_{(x_0,y_0)} 
& = f_x(x_0,y_0) cos \ \alpha + f_y(x_0,y_0) cos \ \beta \\
& = (f_x(x_0,y_0),f_y(x_0,y_0)) · (cos \ \alpha, cos \ \beta) \\
& = (f_x(x_0,y_0),f_y(x_0,y_0)) · \bold{e}_l \\
& = |(f_x(x_0,y_0),f_y(x_0,y_0))| \ cos \ \theta \\ 
\end{align}
\tag{22}
$$
其中$\theta$是$(f_x(x_0,y_0),f_y(x_0,y_0))$和$\bold{e}_l$的交角。上式(22)表明了函数在这一点的方向导数与向量$(f_x(x_0,y_0),f_y(x_0,y_0))$之间的关系。我们将向量$(f_x(x_0,y_0),f_y(x_0,y_0))$称为函数$f(x,y)$在点$P_0(x_0,y_0)$的梯度，记作$\bold{grad} \ f(x_0,y_0)$或$\nabla f(x_0,y_0)$。特别的有：

* 当方向$\bold{e}_l$与梯度$\bold{grad} \ f(x_0,y_0)$方向一致时，函数$f(x,y)$增加最快，即函数在这个方向的方向导数取得最大值，这个最大值为梯度$\bold{grad} \ f(x_0,y_0)$的模；
* 当方向$\bold{e}_l$与梯度$\bold{grad} \ f(x_0,y_0)$方向相反时，函数$f(x,y)$减少最快，即函数在这个方向的方向导数取得最小值，这个最小值为梯度$\bold{grad} \ f(x_0,y_0)$的模的相反数；
* 当方向$\bold{e}_l$与梯度$\bold{grad} \ f(x_0,y_0)$方向正交时，函数$f(x,y)$的变化率为零。

在同济大学的《高等数学》中，对二元函数的**梯度**，给出如下定义：

> ##### 【定义 7】梯度（来自同济大学《高等数学》第七版下册 P. 106）
>
> 设函数$f(x,y)$在平面区域$D$内具有一阶连续偏导数，则对于每一点$P_0(x_0,y_0) \in D$，都可定出一个向量
>
> $$
> f_x(x_0,y_0) \bold{i} + f_y(x_0,y_0) \bold{j} \tag{22}
> $$
>
> 这向量称为函数$f(x,y)$在点$P_0(x_0,y_0)$的梯度，记作$\bold{grad} \ f(x_0,y_0)$或$\nabla f(x_0,y_0)$，即
> $$
> \bold{grad} \ f(x_0,y_0) = \nabla f(x_0,y_0) = f_x(x_0,y_0) \bold{i} + f_y(x_0,y_0) \bold{j} \tag{23}
> $$
>
> 其中$\nabla = \frac{\partial}{\partial x} \bold{i} + \frac{\partial}{\partial y} \bold{j}$称为（二维的）向量微分算子或Nabla算子，$\nabla f = \frac{\partial f}{\partial x} \bold{i} + \frac{\partial f}{\partial y} \bold{j}$。

其中$\bold{i}$、$\bold{j}$分别为与第一坐标轴同方向和与第二坐标轴同方向的单位向量。

综上所述，我们得出：多元函数中，某点目标函数值下降最快的方向，就是该点目标函数的梯度的反方向。

## 三、确定迭代距离：梯度下降法和最速下降法

这在这一节中，我们讨论迭代距离的计算问题。分别介绍利用瞬时变化率确定步长的**梯度下降法**和利用一维搜索确定步长的**最速下降法**。而后讨论在具体实现以上方法时会出现的问题，包括未知目标函数的一阶导数计算以及一维搜索的实现。

#### 1. 步长

在上一节中，我们已经算出了目标函数值下降最快的方向。下面讨论每次迭代移动的距离——**步长**。

下面我们以一元函数$f(x)=x^2$作为目标函数，以$x=2$作为初始值，来看一下步长对于迭代的影响。

##### (1) 固定步长

当步长取固定值时，步长与$x$每次迭代的值如下表（表3）所示。

| 迭代次数                  | 0    | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    |
| ------------------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| **步长=0.3时，x的迭代值** | 2.0  | 1.7  | 1.4  | 1.1  | 0.8  | 0.5  | 0.2  | -0.1 | 0.2  | ……   |
| **步长=0.5时，x的迭代值** | 2.0  | 1.5  | 1.0  | 0.5  | 0.0  |      |      |      |      |      |
| **步长=0.7时，x的迭代值** | 2.0  | 1.3  | 0.6  | -0.1 | 0.6  | ……   |      |      |      |      |
| **步长=5时，x的迭代值**   | 2.0  | -3.0 | 2.0  | ……   |      |      |      |      |      |      |

<center>表3</center>

通过上表，我们可以发现：

* 当步长为固定值时，极大概率无法收敛，而是会进入振幅恒定的震荡；且步长越大，振幅越大。
* 在没有进入震荡前，步长越大，目标函数值的下降速度越快；步长越小，目标函数值下降速度越慢。

##### (2) 依据瞬时变化率确定步长

根据对固定步长的讨论，我们希望：在距离极值点较远时，步长取得更大一些，使算法收敛更快；而当距离极值点较近时，步长取得更小一些，避免在极小值点产生震荡。

通常来说，瞬时变化率越大，则目标函数越陡峭，距离极值点也越远；瞬时变化率越小，则目标函数越平缓，距离极值点也越近；瞬时变化率近似为零，则目标函数极为平缓，已非常靠近极小值点。因此，瞬时变化率的变化规律与我们对步长变化规律的希望相似。于是，考虑用瞬时变化率来决定步长，使步长与瞬时变化率保持**正比例关系**，即点$P(x_k)$，$x_k \in R^n$的步长为
$$
\lambda_k = \eta ( - \nabla f(x_k)) \tag{24}
$$

其中$\eta$为一个常数，称为**学习率**。学习率与$x$每次迭代的值如下表（表4）所示。

| 迭代次数                    | 0      | 1       | 2      | 3       | 4      | 5       | 6      | 7       | 8      | 9       |
| --------------------------- | ------ | ------- | ------ | ------- | ------ | ------- | ------ | ------- | ------ | ------- |
| **学习率=0.1时，x的迭代值** | 2.0000 | 1.6000  | 1.2800 | 1.0240  | 0.8192 | 0.6554  | 0.5243 | 0.4194  | 0.3355 | 0.2684  |
| **学习率=0.3时，x的迭代值** | 2.0000 | 0.8000  | 0.3200 | 0.1280  | 0.0512 | 0.0205  | 0.0082 | 0.0033  | 0.0013 | 0.0005  |
| **学习率=0.5时，x的迭代值** | 2.0000 | -0.0000 | 已收敛 |         |        |         |        |         |        |         |
| **学习率=0.7时，x的迭代值** | 2.0000 | -0.8000 | 0.3200 | -0.1280 | 0.0512 | -0.0205 | 0.0082 | -0.0033 | 0.0013 | -0.0005 |
| **学习率=1.1时，x的迭代值** | 2.00   | -2.40   | 2.88   | -3.46   | 4.15   | -4.98   | 5.97   | -7.17   | 8.60   | -10.32  |

<center>表4</center>

通常上表，我们可以发现：

* 学习率较小，每一次迭代移动距离越短，越不容易进入震荡，但是收敛速度也越慢
* 学习率较大，每一次迭代移动距离越长，越容易进入震荡，但是收敛速度也越快（如果可以收敛的话）
* 当学习率过大，每一次迭代移动距离过长，会导致进入振幅越来越大的震荡，无法收敛

这种通过瞬时变化率乘以**学习率**系数来确定步长的迭代方法，就是**梯度下降法**。

---

在此基础上，还有如下进一步优化的想法：

**衰减**（减缓震荡）：随着迭代过程的继续，学习率应当适当减小，从而更加稳妥地到达极值点，这种想法通过权重衰减因子实现，步长为
$$
\lambda_k = \frac{\eta ( - \nabla f(x_k))}{1+decay×i} \tag{25}
$$
其中$decay$为衰减因子，$i$为当前迭代周期。这样，随着迭代周期的增加，学习率将逐渐减小。衰减因子越大，学习率减小的速度越快；衰减因子越小，学习率减小的速度越慢。

**冲量**（加速迭代过程）：随着迭代过程的进行，瞬时变化率会逐渐，目标函数值的下降速度也会逐渐减慢。因此，考虑在当前迭代过程中考虑前一次迭代的一部分影响，从而加速迭代过程。这种想法的实现通过冲量实现，步长为
$$
\lambda_k = \eta ( - \nabla f(x_k)) + momentum × \eta ( - \nabla f(x_{k-1})) \tag{26}
$$
其中$\nabla f(x_{k-1})$为上一次迭代的瞬时变化率，$momentum$为冲量，取值范围为$[0,1]$。冲量越大，当前迭代过程受前一次迭代的影响越大；冲量越小，当前迭代过程受前一次迭代的影响越小。

##### (3) 依据一维搜索确定步长

使用瞬时变化率确定步长的方法，终究很难避免震荡的出现，于是我们有了另一种想法：每次确定迭代方向后，直接移动到当前方向的**极值点**（驻点），从而避免在当前方向上的反复震荡。

这种方法需要在每次确定迭代方向后进行**一维搜索**，寻找当前方向上的极值点。一维搜索可以使用黄金分割法、斐波那契数列法、二分法、割线法、牛顿法等方法。

这种通过一维搜索寻找当前方向极值点确定步长的方法，就是**最速下降法**。

#### 2. 梯度下降法：瞬时变化率确定步长

**梯度下降法**的步长通过学习率和瞬时变化率确定，根据式(24)，在第$k$次迭代周期中，其步长为
$$
\lambda_k = \eta ( - \nabla f(x_k)) \tag{27}
$$
自变量在迭代过程中有更新方法
$$
x_{k+1} = x_k + \eta ( - \nabla f(x_k)) \tag{28}
$$
梯度下降法算法如下：

> ##### 【算法1】梯度下降法
>
> 输入：目标函数$f(x)$，学习率$\eta$，计算精度$\epsilon$；
>
> 输出：$f(x)$的极小值点$x^*$。
>
> (1) 取初值$x^{(0)} \in R^n$，置$k=0$。
>
> (2) 计算$f(x^{(k)})$。
>
> (3) 计算梯度$\nabla f(x^{(k)})$，令$\lambda_k = - \eta \nabla f(x^{(k)})$。
>
> (4) 置$x^{(k+1)} = x^{(k)} + \lambda_k$，计算$f(x^{(k+1)})$。当$||f(x^{(k+1)})-f(x^{(k)})||<\epsilon$时，停止迭代，令$x^* = x^{(k+1)}$；否则，置$k=k+1$，转至步骤(3)。

#### 3. 最速下降法：一维搜索确定步长

**最速下降法**的步长通过一维搜索寻找当前方向极值点来确定，即求$\lambda_k$使得
$$
f(x_k + \lambda_k (-\nabla f(x_k)) = \min_{\lambda \ge 0} f(x_k + \lambda (-\nabla f(x_k)) \tag{29}
$$
最速下降法算法如下：

> ##### 【算法2】最速下降法
>
> 输入：目标函数$f(x)$，计算精度$\epsilon$；
>
> 输出：$f(x)$的极小点$x^*$。
>
> (1) 取初始值$x^{(0)} \in R^n$，置$k=0$。
>
> (2) 计算$f(x^{(k)})$。
>
> (3) 计算梯度$\nabla f(x^{(k)})$，当$||\nabla f(x^{(k)})||<\epsilon$时，停止迭代，令$x^*=x^{(k)}$；否则，令$p_k=-\nabla f(x^{(k)})$，求$\lambda_k$，使
> $$
> f(x^{(k)}+\lambda_k p_k) = \min_{\lambda \ge 0} f(x^{(k)}+\lambda p_k) \tag{30}
> $$
>
> (4) 置$x^{(k+1)}=x^{(k)} + \lambda_k p_k$，计算$f(x^{(k+1)})$。当$||f(x^{(k+1)})-f(x^{(k)})||<\epsilon$或$||x^{(k+1)}-x^{(k)}||<\epsilon$时，停止迭代，令$x^*=x^{(k+1)}$；否则，置$k=k+1$，转至步骤(3)。

因为还涉及到一维搜索寻找极值点的问题，所以我们引入**黄金分割法**。

##### 一维搜索方法：黄金分割法

黄金分割法的基本思想是：准备在区间$[a_0,b_0]$中寻找极小值点，则可在区间中取得两点$a_1 = a_0 + \rho (b_0-a_0)$，$b_1 = b_0 - \rho (b_0-a_0)$，其中$0 \le \rho \le \frac{1}{2}$。当$f(a_1)<=f(b_1)$时，极小值点存在于区间$[a_0,b_1]$中；当$f(a_1)>f(b_1)$时，则极小值点存在于区间$[a_1,b_0]$中；据此不断迭代缩小区间范围。

在计算过程中，为了避免重复地计算$f(a_1)$和$f(b_1)$，我们希望上一次迭代没有用作边界的$a_1$或$b_1$可以作为下一次迭代的$a_1$或$b_1$使用。由此，我们求解$\rho$的值。

不妨设$b_0-a_0 = 1$；并设下一个迭代区间为$[a_0,b_1]$，即$a_1 = b_2$。于是有
$$
\frac{|a_0 a_1|}{|a_1 b_1|} = \frac{|a_0 b_1|}{|b_1 b_0|} \tag{31}
$$
将各点之间的距离代入，得
$$
\frac{\rho}{(1-2\rho)} = \frac{(1-\rho)}{\rho} \tag{32}
$$
解得$\rho_1 = \frac{3+\sqrt{5}}{2}$（舍去），$\rho_2 = \frac{3-\sqrt{5}}{2} ≈ 0.382$。

黄金分割法算法如下：

> ##### 【算法3】一维搜索方法：黄金分割法
>
> 输入：目标函数$f(x)$，搜索区间$[a_0,b_0]$，计算精度$\epsilon$；
>
> 输出：$f(x)$的极小值点$x^*$。
>
> (1) 置$k=0$。
>
> (2) 取区间中的点$a'^{(k)} = a^{(k)} + 0.382 × (b^{(k)}-a^{(k)})$，$b'^{(k)} = b^{(k)} - 0.382 × (b^{(k)} - a^{(k)})$。
>
> (3) 计算$f(a'^{(k)})$和$f(b'^{(k)})$。
>
> (4) 如果$f(a'^{(k)})<=f(b'^{(k)})$，则置
> $$
> \begin{align}
> a^{(k+1)} = a^{(k)}, \hspace{1em} & b^{(k+1)}=b'^{(k)} \\
> a'^{(k+1)} = a^{(k+1)} + 0.382 × (b^{(k+1)}-a^{(k+1)}), \hspace{1em} & b'^{(k+1)} = a'^{(k)} \\
> & f(b'^{(k+1)})=f(a'^{(k)}) \ \hspace{1em}
> \end{align}
> $$
> 并计算$f(a'^{(k+1)})$；
>
> 否则，置
> $$
> \begin{align}
> a^{(k+1)}=a'^{(k)}, \hspace{1em} & b^{(k+1)}=b^{(k)} \\
> a'^{(k+1)} = b'^{(k)}, \hspace{1em} & b'^{(k+1)} = b^{(k+1)} - 0.382 × (b^{(k+1)} - a^{(k+1)}) \\
> f(a'^{(k+1)})=f(b'^{(k)}) \ \hspace{1em} & 
> \end{align}
> $$
> 并计算$f(b'^{(k+1)})$。
>
> (5) 如果$b^{(k+1)} - a^{(k+1)} < \epsilon$，停止迭代，令$x^* = \frac{(a^{(k+1)}+b^{(k+1)})}{2}$；否则，置$k=k+1$，转至步骤(4)。

## 四、代码实现

在前两节中，我们分别讨论了梯度下降法和最速下降法的迭代方向计算方法和迭代距离计算方法。在这一节中，我们将用Python实现前两节的方法。首先，我们实现梯度计算和一维搜索，而后以此实现梯度下降法和最速下降法。

我们定义二元函数$f(x^{(0)},x^{(1)}) = (x^{(0)}+3)^2 + (x^{(1)}+4)^2$作为例子，其中图像如图5所示：

![image-20210418153027540](image-20210418153027540.png)

<center>图5</center>

```python
def foo(arr):
    return ((arr[0] + 3) ** 2 + (arr[1] + 4) ** 2) / 2
```

#### 1. 梯度计算

##### (1) 未知一元函数的导数计算

在具体实现中，由于很多情况下目标函数未知，无法用数学方法求得目标函数在某点的梯度。对于未知的一元函数，我们无法计算函数的导函数，但是我们使用**一元函数的一阶泰勒展开式**可以近似地表示点$P(x_0)$附近的导数
$$
f(x) = f(x_0) + f'(x_0)(x-x_0) + o(x-x_0) \tag{33}
$$
其中$ o(x-x_0)$为比$(x-x_0)$更高阶的无穷小。于是，点$P(x_0)$附近的导数可以通过下式计算：
$$
f'(x_0) ≈ \frac{f(x) - f(x_0)}{x-x_0} = \frac{\Delta y}{\Delta x} \tag{34}
$$

##### (2) 未知多元函数的偏导数计算

将未知一元函数的导数计算方法推广到多元函数，我们有**多元函数的一阶泰勒展开式**，这里我们以二元函数为例。
$$
f(x,y) = f(x_0,y_0) + (x-x_0) f_x(x_0,y_0) + (y-y_0) f_y(x_0,y_0) + o(x-x_0,y-y_0) \tag{35}
$$
当$y=y_0$时，式(35)可以写成
$$
f(x,y_0) = f(x_0,y_0) + (x-x_0) f_x(x_0,y_0) + o(x-x_0) \tag{36}
$$
于是二元函数$z=f(x,y)$在点$(x_0,y_0)$的偏导数可以由下式计算：
$$
f_x(x_0,y_0) = \frac{f(x,y_0)-f(x_0,y_0)}{x-x_0} \tag{37}
$$
同理可得
$$
f_y(x_0,y_0) = \frac{f(x_0,y)-f(x_0,y_0)}{y-y_0} \tag{38}
$$
以上结论可以推广到二元以上的多元函数。

##### (3) 未知多元函数的梯度计算

多元函数的梯度通过偏导数计算，解决了偏导数的计算，即解决了梯度的计算。多元函数$f(x)$，$x \in R^n$在点$P(x_0)$的梯度为
$$
\nabla f(x_0) = (f_{x^{(0)}}(x_0,y_0),f_{x^{(1)}}(x_0,y_0),\cdots,f_{x^{(n)}}(x_0,y_0)) \tag{39}
$$

##### (4) 代码实现

通过逐个自变量使用式(37)计算偏导数，朴素地实现计算未知$n$元函数在某点的所有自变量的偏导数，即梯度向量如下：

```python
def partial_derivative(func, arr, dx=1e-6):
    """计算n元函数在某点的所有自变量的偏导数列表（梯度向量）

    :param func: [function] n元函数
    :param arr: [list/tuple] 目标点的自变量坐标
    :param dx: [int/float] 计算时x的增量
    :return: [list] 偏导数
    """
    ans = []
    for i in range(len(arr)):
        arr2 = list(arr)
        arr2[i] += dx
        ans.append((func(arr2) - func(arr)) / dx)
    return ans

partial_derivative(foo2, [0, 0])  # [3.000000500463784, 4.000000499715384]
```

更优地，我们也可以使用scipy科学计算包中计算一元函数导数的`derivative`函数，这个函数使用了中心差分公式，准确率更高。在具体实现上，我们构造了只沿指定自变量变化的一元函数`f`作为`derivative`函数的参数。

```python
from scipy.misc import derivative

def partial_derivative(func, arr, dx=1e-6):
    """计算n元函数在某点各个自变量的偏导数列表（梯度向量）

    :param func: [function] n元函数
    :param arr: [list/tuple] 目标点的自变量坐标
    :param dx: [int/float] 计算时x的增量
    :return: [list] 偏导数
    """
    n_features = len(arr)
    ans = []
    for i in range(n_features):
        def f(x):
            arr2 = list(arr)
            arr2[i] = x
            return func(arr2)

        ans.append(derivative(f, arr[i], dx=dx))
    return ans

partial_derivative(foo2, [0, 0])  # [3.000000000419334, 3.9999999996709334]
```

#### 2. 基于黄金分隔法的一维搜索实现

依据【算法3】，我们实现了基于黄金分割法的一维搜索方法如下：

```python
def golden_section_for_line_search(func, a0, b0, epsilon):
    """一维搜索极小值点（黄金分割法）

    :param func: [function] 一元函数
    :param a0: [int/float] 目标区域左侧边界
    :param b0: [int/float] 目标区域右侧边界
    :param epsilon: [int/float] 精度
    """
    a1, b1 = a0 + 0.382 * (b0 - a0), b0 - 0.382 * (b0 - a0)
    fa, fb = func(a1), func(b1)

    while b1 - a1 > epsilon:
        if fa <= fb:
            b0, b1, fb = b1, a1, fa
            a1 = a0 + 0.382 * (b0 - a0)
            fa = func(a1)
        else:
            a0, a1, fa = a1, b1, fb
            b1 = b0 - 0.382 * (b0 - a0)
            fb = func(b1)

    return (a1 + b1) / 2

golden_section_for_line_search(foo1, -10, 5, epsilon=1e-6)  # 5.263005013597177e-06
```

#### 3. 梯度下降法实现

依据【算法1】，我们实现梯度下降法如下：

```python
def gradient_descent(func, n, eta, epsilon, maximum=1000):
    """梯度下降法

    :param func: [function] n元目标函数
    :param n: [int] 目标函数元数
    :param eta: [int/float] 学习率
    :param epsilon: [int/float] 学习精度
    :param maximum: [int] 最大学习次数
    :return: [list] 结果点坐标
    """
    x0 = [0] * n  # 取自变量初值
    y0 = func(x0)  # 计算函数值
    for _ in range(maximum):
        nabla = partial_derivative(func, x0)  # 计算梯度
        x1 = [x0[i] - eta * nabla[i] for i in range(n)]  # 迭代自变量
        y1 = func(x1)  # 计算函数值

        if abs(y1 - y0) < epsilon:  # 如果当前变化量小于学习精度，则结束学习
            return x1

        x0, y0 = x1, y1
      
gradient_descent(foo2, 2, eta=0.2, epsilon=1e-6)  # [-2.999026444340054, -3.9987019257868917] (迭代35次)
```

#### 4. 最速下降法实现

依据【算法2】，我们实现最速下降法如下：

```python
def steepest_descent(func, n, epsilon, distance=3, maximum=1000):
    """最速下降法

    :param func: [function] n元目标函数
    :param n: [int] 目标函数元数
    :param epsilon: [int/float] 学习精度
    :param distance: [int/float] 每次一维搜索的长度范围（distance倍梯度的模）
    :param maximum: [int] 最大学习次数
    :return: [list] 结果点坐标
    """
    x0 = [0] * n  # 取自变量初值
    y0 = func(x0)  # 计算函数值
    for _ in range(maximum):
        nabla = partial_derivative(func, x0)  # 计算梯度

        # 当梯度的模长小于精度要求时，停止迭代
        if pow(sum([nabla[i] ** 2 for i in range(n)]), 0.5) < epsilon:
            return x0

        def f(x):
            """梯度方向的一维函数"""
            x2 = [x0[i] - x * nabla[i] for i in range(n)]
            return func(x2)

        lk = golden_section_for_line_search(f, 0, distance, epsilon=1e-6)  # 一维搜索寻找驻点

        x1 = [x0[i] - lk * nabla[i] for i in range(n)]  # 迭代自变量
        y1 = func(x1)  # 计算函数值

        if abs(y1 - y0) < epsilon:  # 如果当前变化量小于学习精度，则结束学习
            return x1

        x0, y0 = x1, y1

steepest_descent(foo2, 2, epsilon=1e-6)  # [-2.9999999999635865, -3.999999999951452] (1次迭代)
```

## 五、参考文献

#### 核心参考文献

1. 《高等数学》第七版 下册 - 同济大学数学系 - 高等教育出版社：第二章 导数
2. 《高等数学》第七版 上册 - 同济大学数学系 - 高等教育出版社：第九章 多元函数微分法及其应用
3. 《统计学习方法》第2版 - 李航 - 清华大学出版社：附录A 梯度下降法

#### 其他参考文献

1. [如何直观形象地理解方向导数与梯度以及它们之间的关系？ - 马同学的回答 - 知乎](https://www.zhihu.com/question/36301367/answer/156102040)
2. [导数，微分，梯度的简单理解 - 一个小迷糊66的文章 - CSDN](https://blog.csdn.net/jianyuchen23/article/details/96965775)
3. [梯度下降法的推导（非常详细、易懂的推导） - /home/liupc的文章 - CSDN](https://blog.csdn.net/pengchengliu/article/details/80932232)
4. [梯度下降法背后的原理 - 公众号 AI末班车的文章 - 知乎](https://zhuanlan.zhihu.com/p/45122093)
5. [梯度下降法和一阶泰勒展开的关系 - 马东什么的文章 - 知乎](https://zhuanlan.zhihu.com/p/82757193)
6. [梯度下降法(Gradient Descent)优化函数的详解（1）梯度下降法（Gradient Descent）- Lane Phoebe的文章 - CSDN](https://blog.csdn.net/qq_25628891/article/details/83195944)
7. [梯度下降算法原理讲解——机器学习 -Arrow and Bullet的文章 - CSDN](https://blog.csdn.net/qq_41800366/article/details/86583789)
8. [梯度下降算法及泰勒展开式 - 古花米草的文章 - 知乎](https://zhuanlan.zhihu.com/p/90276574)
9. [什么是梯度下降法？ - 张戎的回答 - 知乎](https://www.zhihu.com/question/305638940/answer/606831354)
10. [最清晰的讲解各种梯度下降法原理与Dropout - AI火箭营的文章 - 百家号](https://baijiahao.baidu.com/s?id=1613121229156499765)
11. [梯度下降法受收敛性证明 - hobbit的文章 - 知乎](https://zhuanlan.zhihu.com/p/92151073)
12. [【最优化】一文搞懂最速下降法 - 忆臻的文章 - 知乎](https://zhuanlan.zhihu.com/p/32709034)
13. [最速下降法/梯度下降法 - JasonQ_NEU的文章 - CSDN](https://blog.csdn.net/u012430664/article/details/78404844)
14. [【最优化导论】一维搜索方法 - 土豆洋芋山药蛋的文章 - CSDN](https://blog.csdn.net/qq_33414271/article/details/99686934)
16. [全导数、偏导数、方向导数 - Running_Tiger的文章 - CSDN](https://blog.csdn.net/qq_41455420/article/details/79661885)
17. [多元函数的泰勒展开式 - 忆臻的文章 - 知乎](https://zhuanlan.zhihu.com/p/33316479)
18. [牛顿法与拟牛顿法学习笔记（一）牛顿法 - 皮果提的文章 - CSDN](https://blog.csdn.net/itplus/article/details/21896453)
19. [一维搜索方法之割线法（python实现） - 落叶_小唱的文章 - CSDN](https://blog.csdn.net/ouening/article/details/103002653)
20. [利用黄金分割法一维搜索的最速下降法matlab源程序 - 泛舟的文章 - 博客园](https://www.cnblogs.com/lyfruit/archive/2013/01/06/2848426.html)
21. [Gradient-Descent (梯度下降，优化函数大法) - 酷酷的算法](https://mp.weixin.qq.com/s/EXumVg7EPcl0ZeRVeUk82g)
22. [什么是梯度下降法？ - 陈琛的回答 - 知乎](https://www.zhihu.com/question/305638940/answer/770984541)
23. [梯度下降法基本原理 - 奋斗在阿尔卑斯的皮卡丘的文章 - CSDN](https://blog.csdn.net/qq_37289115/article/details/109176741)
23. [梯度下降方法中的学习率(learning rate), 衰减因子(decay) 冲量(momentum) - 17420的文章 - CSDN](https://blog.csdn.net/zsfcg/article/details/90477938)
24. [多元梯度下降法的实践之学习率的选择 - 爱编程的高老师的文章 - 知乎](https://zhuanlan.zhihu.com/p/161378418)
25. [一文简述深度学习优化方法——梯度下降 - 机器之心的文章 - 知乎](https://zhuanlan.zhihu.com/p/39842768)
26. [python中计算梯度值 - Legolas~文章 - CSDN](https://blog.csdn.net/qq_38883271/article/details/103423409)
27. [最速下降法（梯度下降法）python实现 - Tomator01文章 - CSDN](https://blog.csdn.net/Big_Pai/article/details/88539543)

